




















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# FILIPPO: df = pd.read_csv('../../data_set/train-data.csv', index_col=0)
# CICCIO:
df = pd.read_csv('../../data_set/train-data.csv', index_col=0)

df.head()





df.info()
































print(df.isna().sum())

print("")

print("---- NaN percentage per variable ----")
for x in df:
    if df[x].isnull().sum() > 0:
        value = (df[x].isna().sum()/len(df[x])) * 100
        print(f"{x}: {value:.2f}%")








duplicate_count = df.duplicated().sum()

df.drop_duplicates(inplace=True)

print(f"Rimosse di n. righe duplicate: {duplicate_count}")





















df['Brand'] = df['Name'].str.split(' ').str[0]
df['Model'] = df['Name'].str.split(' ', n=1).str[1]
df = df.drop('Name', axis=1)

first_col = ['Brand', 'Model']

remaining_cols = [col for col in df.columns if col not in first_col]

df = df[first_col + remaining_cols]

print("Brand e Model creati e inseriti come prime colonne.")

df = df.reset_index(drop=True)

df.head()





df['Brand'].unique()


df['Model'].unique()





df['Brand'].value_counts()








def clean_formatting(val):
    if isinstance(val, str):
        return val.strip().lower().capitalize()
    return val

df['Brand'] = df['Brand'].apply(clean_formatting)

duplicates_count = df.duplicated().sum()
df = df.drop_duplicates()

df = df.reset_index(drop=True)

print(f"Pulizia completata: rimossi {duplicates_count} duplicati.")


df['Brand'] = df['Brand'].replace({'Land': 'Land Rover'})
brand_counts = df['Brand'].value_counts()

rare_brands = brand_counts[brand_counts <= 60].index

luxury_list = [
    'Lamborghini', 'Bentley', 'Porsche', 'Jaguar', 'Land Rover', 'Volvo', 'Mercedes-Benz', 'Audi', 'BMW', 'Lexus', 'Jeep'
]

def clean_brand_smart(brand):
    if brand not in rare_brands:
        return brand
    else:
        if brand in luxury_list:
            return 'Other_Luxury' 
        else:
            return 'Other'         

df['Brand'] = df['Brand'].apply(clean_brand_smart)

print("Distribuzione Brand dopo il raggruppamento:")
print(df['Brand'].value_counts())
df.reset_index(drop=True, inplace=True)

















print(df['Location'].unique())
print("")
print(df['Location'].value_counts())




















current_year = 2019

index_year = df.columns.get_loc('Year')
age_values = current_year - df['Year']
df.insert(index_year, 'Age', age_values)
df.drop('Year', axis=1, inplace=True)

print('Age creata.')








df['Age'].plot.box(figsize=(2,4))





df[df['Age'] >= 15]['Age'].value_counts()





df[df['Age'] == 18].head(8)


df[df['Age'] > 19].head(10)


df[df['Age'] == 18].head(8)





threshold = 14
auto_vecchie = df[df['Age'] > threshold]

df = df[df['Age'] <= threshold]
df.reset_index(drop=True, inplace=True)

print(f"Numero di auto rimosse: {len(auto_vecchie)}")




















df['Kilometers_Driven'].plot.box(figsize=(2,4))





print("Outlier estremi:")
print(df[df['Kilometers_Driven'] >= 1000000])

df = df[df['Kilometers_Driven'] < 1000000]
df.reset_index(drop=True, inplace=True)





df['Kilometers_Driven'].plot.box(figsize=(2,4))





soglia_99 = df['Kilometers_Driven'].quantile(0.99)
print(f"Soglia (99%): {soglia_99:.0f} km")

df['Kilometers_Driven'] = np.where(df['Kilometers_Driven'] > soglia_99, 
                                   soglia_99, 
                                   df['Kilometers_Driven'])

df.reset_index(drop=True, inplace=True)

df['Kilometers_Driven'].plot.box(figsize=(2,6))
plt.grid()
plt.show()

















df['Fuel_Type'].value_counts()





df = df[(df['Fuel_Type'] != 'Electric') & (df['Fuel_Type'] != 'CNG') & (df['Fuel_Type'] != 'LPG')]

df = df.reset_index(drop=True)

print("Nicchie rimosse.")

















df['Transmission'].unique()




















df['Owner_Type'].unique()


df['Owner_Type'].value_counts()





df['Owner_Type'] = df['Owner_Type'].replace('Fourth & Above', 'Third')














print("Valori mancanti:", df['Mileage'].isna().sum())








df_mileage = df['Mileage'].dropna()

# Controllo sulla formattazione
pattern = r'^\S+\s\S+$'

risultato_controllo = df_mileage.astype(str).str.match(pattern)

total_obs = len(df_mileage)
conformi = risultato_controllo.sum()
non_conformi = total_obs - conformi

print(f"Conformi: {conformi}")
print(f"Non conformi: {non_conformi}")

# Controllo sulle unità
mileage_units = df_mileage.str.split().str[-1]

print("\n---- Unità di misura presenti ----")
mileage_units.unique()








df['Mileage'] = df['Mileage'].str.split(' ').str[0]
df['Mileage'] = pd.to_numeric(df['Mileage'], errors='coerce')

df['Mileage'] = df['Mileage'].astype('int64')

print("Valori estratti.")








df['Mileage'].plot.box(figsize=(2,4))





print(f"Valori per mileage inaccettabili: {len(df[df['Mileage'] <= 0])}")





df = df[df['Mileage'] > 0]
df = df.reset_index(drop=True)

print("Righe rimosse.")














print("Valori mancanti:", df['Engine'].isna().sum())





df = df[~df['Engine'].isna()].reset_index(drop=True)

print("Righe rimosse.")








df_engine = df['Engine']

# Controllo sulla formattazione
pattern = r'^\S+\s\S+$'

risultato_controllo = df_engine.astype(str).str.match(pattern)

total_obs = len(df_engine)
conformi = risultato_controllo.sum()
non_conformi = total_obs - conformi

print(f"Conformi: {conformi}")
print(f"Non conformi: {non_conformi}")

# Controllo sulle unità
engine_units = df_engine.str.split().str[-1]

print("\n---- Unità di misura presenti ----")
engine_units.unique()


df['Engine'] = df['Engine'].str.split(' ').str[0]
df['Engine'] = pd.to_numeric(df['Engine'], errors='coerce')

df['Engine'] = df['Engine'].astype('int64')

print("Valori estratti.")








df['Engine'].plot.box(figsize=(2,4))














print(f"Valori mancanti: {df['Power'].isna().sum()}")











df_power = df['Power']

# Controllo sulla formattazione
pattern = r'^\S+\s\S+$'

risultato_controllo = df_power.astype(str).str.match(pattern)

total_obs = len(df_power)
conformi = risultato_controllo.sum()
non_conformi = total_obs - conformi

print(f"Conformi: {conformi}")
print(f"Non conformi: {non_conformi}")

# Controllo sulle unità
power_units = df_power.str.split().str[-1]

print("\n---- Unità di misura presenti ----")
power_units.unique()


pattern_numerico = r'.*\d+.*'

# 1. Identifica le stringhe che NON contengono un numero
# Il .astype(str) è importante per catturare tutti i tipi di stringa
non_numeriche_mask = ~df['Power'].astype(str).str.contains(pattern_numerico, na=False)

# 2. Converti le stringhe non numeriche identificate in NaN
df.loc[non_numeriche_mask, 'Power'] = np.nan

print(f"Smascherati {non_numeriche_mask.sum()} valori senza numeri come NaN.")


df = df[~df['Power'].isna()].reset_index(drop=True)

print("Righe rimosse.")





df['Power'] = df['Power'].str.split(' ').str[0]
df['Power'] = pd.to_numeric(df['Power'], errors='coerce')

df['Power'] = df['Power'].astype('float64')

print("Valori estratti.")








df['Power'].plot.box(figsize=(2,4))














print(f"Valori mancanti: {df['Seats'].isna().sum()}")





df['Seats'].value_counts()








df = df[df['Seats'] <= 8]
df = df.reset_index(drop=True)


mediana_seats = df['Seats'].median()
df['Seats'] = df['Seats'].fillna(mediana_seats)

print("Valori imputati.")


df['Seats'] = df['Seats'].astype(int)








df['Seats'].plot.box(figsize=(2,4))











def get_cat(seats):
    if seats <= 2: return 'Sports'
    if seats <= 5: return 'Standard'
    return 'Large'

df['Cat'] = df['Seats'].apply(get_cat)
df = df.drop('Seats', axis=1)











total = len(df)

print(f"Valori mancanti: {df['New_Price'].isna().sum()} ({df['New_Price'].isna().sum() / total:.2%})")
print(f"Valori non mancanti: {df['New_Price'].notnull().sum()} ({df['New_Price'].notnull().sum() / total:.2%})")





import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="whitegrid")
viridis_pal = sns.color_palette("viridis", as_cmap=False)

temp_df = df.copy()
temp_df['New_Price_Status'] = temp_df['New_Price'].notnull().map({True: 'Has Price', False: 'No Price'})

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Correzione: aggiunto hue e legend=False per evitare il FutureWarning
sns.barplot(data=temp_df, x='New_Price_Status', y='Age', ax=axes[0], hue='New_Price_Status', palette="viridis", errorbar=None, legend=False)
axes[0].set_title('Confronto Età Media', fontsize=14, fontweight='bold')
axes[0].set_xlabel('')
axes[0].set_ylabel('Età (Anni)')

sns.countplot(data=temp_df, x='New_Price_Status', hue='Transmission', ax=axes[1], palette="viridis")
axes[1].set_title('Distribuzione Cambio', fontsize=14, fontweight='bold')
axes[1].set_xlabel('')
axes[1].set_ylabel('Numero di Auto')

owner_order = ['First', 'Second', 'Third', 'Fourth & Above']
sns.countplot(data=temp_df, x='New_Price_Status', hue='Owner_Type', hue_order=owner_order, ax=axes[2], palette="viridis")
axes[2].set_title('Distribuzione Proprietari', fontsize=14, fontweight='bold')
axes[2].set_xlabel('')
axes[2].set_ylabel('Numero di Auto')

plt.tight_layout()
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="whitegrid")

temp_df = df.copy()
temp_df['New_Price_Status'] = temp_df['New_Price'].notnull().map({True: 'Has Price', False: 'No Price'})

fig, axes = plt.subplots(2, 2, figsize=(18, 12))

# --- 1. BRAND CATEGORY (Countplot) ---
sns.countplot(data=temp_df, x='Brand', hue='New_Price_Status', ax=axes[0, 0], palette="viridis")
axes[0, 0].set_title('New Price Status per Brand', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].set_ylabel('Conteggio Auto')

# --- 2. POWER (Bar Plot - Potenza Media) ---
sns.barplot(data=temp_df, x='New_Price_Status', y='Power', ax=axes[0, 1], hue='New_Price_Status', palette="viridis", errorbar=None, legend=False)
axes[0, 1].set_title('Potenza Media (Power) per Stato Prezzo', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('')
axes[0, 1].set_ylabel('Media Cavalli (BHP)')

# --- 3. FUEL TYPE (Countplot) ---
sns.countplot(data=temp_df, x='Fuel_Type', hue='New_Price_Status', ax=axes[1, 0], palette="viridis")
axes[1, 0].set_title('Disponibilità per Tipo Carburante', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('')
axes[1, 0].set_ylabel('Conteggio Auto')

# --- 4. LOCATION (Countplot) ---
sns.countplot(data=temp_df, x='Location', hue='New_Price_Status', ax=axes[1, 1], palette="viridis")
axes[1, 1].set_title('Analisi Geografica (Location)', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].set_ylabel('Conteggio Auto')

plt.tight_layout()
plt.show()











df['Has_New_Price'] = df['New_Price'].notnull().astype(int)

print(df.groupby('Has_New_Price')['Price'].mean())





df[df['New_Price'].notnull()].info()





print(df.groupby('Has_New_Price')['Age'].mean())








temp_series = df['New_Price'].dropna()
units = temp_series.astype(str).apply(lambda x: x.split()[-1].strip())
print("Unità di misura trovate e loro frequenza:")
print(units.value_counts())





def clean_new_price(value):
    if pd.isna(value):
        return 0
    
    value = str(value).strip()
    
    if 'Cr' in value:
        # Rimuove 'Cr', converte in float e moltiplica per 100 
        return float(value.replace('Cr', '').strip()) * 100 
    
    # Gestione Lakh unità standard
    elif 'Lakh' in value:
        return float(value.replace('Lakh', '').strip())
    
    return 0

df['New_Price_num'] = df['New_Price'].apply(clean_new_price)
df['Has_New_Price'] = df['New_Price'].notnull().astype(int)

print(df[['New_Price', 'New_Price_num']].dropna().head())


df = df.drop(['New_Price'], axis=1)




















df['Price'].plot.box(figsize=(2,4))





df[df['Price'] > 100]





df[df['Price'] < 0.5]











df.info()














tabella_brand = pd.crosstab(index=df['Brand'], columns='Frequenza Assoluta')
tabella_brand['Frequenza Relativa (%)'] = (tabella_brand['Frequenza Assoluta'] / tabella_brand['Frequenza Assoluta'].sum() * 100).round(2)
tabella_brand = tabella_brand.sort_values(by='Frequenza Assoluta', ascending=False)
tabella_brand['Frequenza Cumulata (%)'] = tabella_brand['Frequenza Relativa (%)'].cumsum().round(2)

display(tabella_brand)








counts_all = df['Brand'].value_counts()
rel_freq_all = df['Brand'].value_counts(normalize=True) * 100

mask_others = rel_freq_all < 3
counts_pie = counts_all[~mask_others].copy()
if mask_others.any():
    counts_pie['Other_Brands'] = counts_all[mask_others].sum()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 6))

sns.barplot(x=counts_all.index, y=counts_all.values, hue=counts_all.index, palette='viridis', legend=False, ax=ax1)
for i, container in enumerate(ax1.containers):
    lbl = f'{(counts_all.values[i] / counts_all.sum() * 100):.2f}%'
    ax1.bar_label(container, labels=[lbl], padding=3, fontsize=9)

ax1.set_title('Brand: Frequenze Assolute e Relative')
ax1.tick_params(axis='x', rotation=45)
ax1.set_ylabel('Numero di Auto')
ax1.set_xlabel('Brand')

wedges, texts, autotexts = ax2.pie(counts_pie, labels=counts_pie.index, autopct='%1.1f%%', 
                                  startangle=140, pctdistance=0.85,
                                  colors=sns.color_palette('viridis', len(counts_pie)))

plt.setp(autotexts, size=9, weight="bold", color="white")
ax2.set_title('Distribuzione Percentuale (Soglia 3%)')

plt.tight_layout()
plt.show()











counts_model = df['Model'].value_counts().head(15)
total_count = len(df)

plt.figure(figsize=(17, 6))
sns.barplot(x=counts_model.index, y=counts_model.values, hue=counts_model.index, palette='viridis', legend=False)

for i, val in enumerate(counts_model.values):
    pct = (val / total_count) * 100
    plt.text(i, val + 0.5, f'{pct:.2f}%', ha='center', fontsize=9)

plt.title('Top 15 Modelli: Frequenze Assolute e Percentuale')
plt.xticks(rotation=45)
plt.ylabel('Numero di Auto')
plt.xlabel('Modello')
plt.tight_layout()
plt.show()








tabella_brand = pd.crosstab(index=df['Location'], columns='Frequenza Assoluta')
tabella_brand['Frequenza Relativa (%)'] = (tabella_brand['Frequenza Assoluta'] / tabella_brand['Frequenza Assoluta'].sum() * 100).round(2)
tabella_brand = tabella_brand.sort_values(by='Frequenza Assoluta', ascending=False)
tabella_brand['Frequenza Cumulata (%)'] = tabella_brand['Frequenza Relativa (%)'].cumsum().round(2)

display(tabella_brand)


count_assoluto = df['Location'].value_counts()
count_relativo = (df['Location'].value_counts(normalize=True) * 100).loc[count_assoluto.index] 

plt.figure(figsize=(17, 6))

ax = sns.barplot(x=count_assoluto.index, y=count_assoluto.values, hue=count_assoluto.index, palette='viridis', legend=False)

for i, container in enumerate(ax.containers):
    percentuale = count_relativo.iloc[i] 
    
    ax.bar_label(container, labels=[f'{percentuale:.2f}%'], padding=3, fontsize=10)

plt.title('Location: Frequenze Assolute con Percentuale')
plt.xticks(rotation=45)
plt.xlabel('Location')

plt.tight_layout()
plt.show()











col = df['Age']

print(col.describe())

print("")

print("mode: ", col.mode())

print("")

print(f"Skewness: {col.skew():.2f}")
print(f"Kurtosis: {col.kurtosis():.2f}")








count_assoluto = df['Age'].value_counts().sort_index()
total = count_assoluto.sum()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 8))

sns.barplot(
    x=count_assoluto.index, 
    y=count_assoluto.values, 
    hue=count_assoluto.index, 
    palette='viridis', 
    legend=False, 
    ax=ax1
)

for container in ax1.containers:
    labels = [f'{(v/total*100):.2f}%' for v in container.datavalues]
    ax1.bar_label(container, labels=labels, padding=3, fontsize=9)

ax1.set_title('Anno: Frequenze Assolute e Percentuale')
ax1.set_xlabel('Età (Anni)')
ax1.set_ylabel('Numero di Auto')

sns.histplot(df['Age'], kde=True, color='skyblue', bins=20, ax=ax2, stat="density")
ax2.set_title('Anno: Distribuzione  con KDE (Densità)')
ax2.set_xlabel('Età (Anni)')
ax2.set_ylabel('Densità')

plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 4))

sns.set_style("whitegrid")

ax = sns.boxplot(x=df['Age'], color='#69b3a2', width=0.5, linewidth=2, fliersize=5)
sns.stripplot(x=df['Age'], color='#404040', alpha=0.3, size=4, jitter=True)

plt.title('Anno: BoxPlot', fontsize=15, pad=20)
plt.xlabel('Età (Anni)', fontsize=12)

sns.despine(left=True)

plt.tight_layout()
plt.show()





col = df['Kilometers_Driven']

print(col.describe())

print("")

print("mode: ", col.mode())

print("")

print(f"Skewness: {col.skew():.2f}")
print(f"Kurtosis: {col.kurtosis():.2f}")








col = df['Kilometers_Driven']

n = len(col)
rice_bins = int(2 * n**(1/3))

print(f"Num. of bins: {rice_bins}")

plt.figure(figsize=(17, 7))


ax = sns.histplot(
    x=col, 
    bins=rice_bins, 
    kde=True, 
    stat='probability',
    color="#108564",
    edgecolor='white'
)
plt.title(f'Distribuzione di Kilometers_Driven')
plt.ylabel('Frequenza Relativa')
plt.xlabel('Kilometers_Driven')

plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 4))

sns.set_style("whitegrid")

ax = sns.boxplot(x=df['Kilometers_Driven'], color='#69b3a2', width=0.5, linewidth=2, fliersize=5)
sns.stripplot(x=df['Kilometers_Driven'], color='#404040', alpha=0.3, size=4, jitter=True)

plt.title('K_Driven: BoxPlot', fontsize=15, pad=20)
plt.xlabel('K_Driven (Km)', fontsize=12)

sns.despine(left=True)

plt.tight_layout()
plt.show()


plt.figure(figsize=(10, 6))
sns.ecdfplot(data=df, x='Kilometers_Driven', color='#69b3a2', linewidth=2)
plt.title('Distribuzione Cumulativa (ECDF) di K_Driven', fontsize=14)
plt.xlabel('K_Driven')
plt.ylabel('Proporzione (0 a 1)')
plt.grid(True, alpha=0.3)
plt.show()











tabella_brand = pd.crosstab(index=df['Fuel_Type'], columns='Frequenza Assoluta')
tabella_brand['Frequenza Relativa (%)'] = (tabella_brand['Frequenza Assoluta'] / tabella_brand['Frequenza Assoluta'].sum() * 100).round(2)
tabella_brand = tabella_brand.sort_values(by='Frequenza Assoluta', ascending=False)
tabella_brand['Frequenza Cumulata (%)'] = tabella_brand['Frequenza Relativa (%)'].cumsum().round(2)

display(tabella_brand)


counts = df['Fuel_Type'].value_counts()
df_stacked = pd.DataFrame(counts).T

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

df_stacked.plot(kind='bar', stacked=True, ax=ax1, color=sns.color_palette('viridis', len(counts)))
ax1.set_title(' Fuel Type: Distribuzione (Stacked Bar)')
ax1.set_ylabel('Numero di Auto')
ax1.set_xticks([])
ax1.legend(title='Fuel Type')

ax2.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, 
        colors=sns.color_palette('viridis', len(counts)), pctdistance=0.85)
ax2.add_artist(plt.Circle((0,0), 0.70, fc='white'))
ax2.set_title('Fuel Type: Fuel Type (Percentuale)')

plt.tight_layout()
plt.show()











tabella_brand = pd.crosstab(index=df['Transmission'], columns='Frequenza Assoluta')
tabella_brand['Frequenza Relativa (%)'] = (tabella_brand['Frequenza Assoluta'] / tabella_brand['Frequenza Assoluta'].sum() * 100).round(2)
tabella_brand = tabella_brand.sort_values(by='Frequenza Assoluta', ascending=False)
tabella_brand['Frequenza Cumulata (%)'] = tabella_brand['Frequenza Relativa (%)'].cumsum().round(2)

display(tabella_brand)


counts = df['Transmission'].value_counts()
df_stacked = pd.DataFrame(counts).T

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

df_stacked.plot(kind='bar', stacked=True, ax=ax1, color=sns.color_palette('viridis', len(counts)))
ax1.set_title(' Fuel Type: Distribuzione (Stacked Bar)')
ax1.set_ylabel('Numero di Auto')
ax1.set_xticks([])
ax1.legend(title='Trabsmission')

ax2.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, 
        colors=sns.color_palette('viridis', len(counts)), pctdistance=0.85)
ax2.add_artist(plt.Circle((0,0), 0.70, fc='white'))
ax2.set_title('Fuel Type: Fuel Type (Percentuale)')

plt.tight_layout()
plt.show()








tabella_brand = pd.crosstab(index=df['Owner_Type'], columns='Frequenza Assoluta')
tabella_brand['Frequenza Relativa (%)'] = (tabella_brand['Frequenza Assoluta'] / tabella_brand['Frequenza Assoluta'].sum() * 100).round(2)
tabella_brand = tabella_brand.sort_values(by='Frequenza Assoluta', ascending=False)
tabella_brand['Frequenza Cumulata (%)'] = tabella_brand['Frequenza Relativa (%)'].cumsum().round(2)

display(tabella_brand)


counts_all = df['Owner_Type'].value_counts()
rel_freq_all = df['Owner_Type'].value_counts(normalize=True) * 100

mask_others = rel_freq_all < 3
counts_pie = counts_all[~mask_others].copy()
if mask_others.any():
    counts_pie['Other'] = counts_all[mask_others].sum()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 6))

sns.barplot(x=counts_all.index, y=counts_all.values, hue=counts_all.index, palette='viridis', legend=False, ax=ax1)
for i, container in enumerate(ax1.containers):
    lbl = f'{(counts_all.values[i] / counts_all.sum() * 100):.2f}%'
    ax1.bar_label(container, labels=[lbl], padding=3, fontsize=9)

ax1.set_title('Owner Type: Frequenze Assolute e Relative')
ax1.tick_params(axis='x', rotation=0)
ax1.set_ylabel('Numero di Auto')
ax1.set_xlabel('Owner Type')

ax2.pie(counts_pie, labels=counts_pie.index, autopct='%1.1f%%', startangle=140, 
        colors=sns.color_palette('viridis', len(counts_pie)), pctdistance=0.85)
ax2.add_artist(plt.Circle((0,0), 0.70, fc='white'))
ax2.set_title('Owner Type: Distribuzione Percentuale (Soglia 3%)')

plt.tight_layout()
plt.show()











col = df['Mileage']

print(col.describe())

print("")

print("mode: ", col.mode())

print("")

print(f"Skewness: {col.skew():.2f}")
print(f"Kurtosis: {col.kurtosis():.2f}")





col = df['Mileage']

n = len(col)
rice_bins = 23

print(f"Num. of bins: {rice_bins}")

plt.figure(figsize=(17, 7))


ax = sns.histplot(
    x=col, 
    bins=rice_bins, 
    kde=True, 
    stat='probability',
    color="#108564",
    edgecolor='white'
)
plt.title(f'Distribuzione di Mileage')
plt.ylabel('Frequenza Relativa')
plt.xlabel('Mileage: km/l')

plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 4))

sns.set_style("whitegrid")

ax = sns.boxplot(x=df['Mileage'], color='#69b3a2', width=0.5, linewidth=2, fliersize=5)
sns.stripplot(x=df['Mileage'], color='#404040', alpha=0.3, size=4, jitter=True)

plt.title('Mileage: BoxPlot', fontsize=15, pad=20)
plt.xlabel('Mileage (Km/l)', fontsize=12)

sns.despine(left=True)

plt.tight_layout()
plt.show()





plt.figure(figsize=(10, 6))
sns.ecdfplot(data=df, x='Mileage', color='#69b3a2', linewidth=2)
plt.title('Distribuzione Cumulativa (ECDF) di Mileage', fontsize=14)
plt.xlabel('Mileage')
plt.ylabel('Proporzione (0 a 1)')
plt.grid(True, alpha=0.3)
plt.show()











col = df['Engine']

print(col.describe())

print("")

print("mode: ", col.mode())

print("")

print(f"Skewness: {col.skew():.2f}")
print(f"Kurtosis: {col.kurtosis():.2f}")





col = df['Engine']

n = len(col)
rice_bins =  40

print(f"Num. of bins: {rice_bins}")

plt.figure(figsize=(17, 7))


ax = sns.histplot(
    x=col, 
    bins=rice_bins, 
    kde=True, 
    stat='probability',
    color="#108564",
    edgecolor='white'
)
plt.title(f'Distribuzione di Engine')
plt.ylabel('Frequenza Relativa')
plt.xlabel('Engine (CC)')

plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 4))

sns.set_style("whitegrid")

ax = sns.boxplot(x=df['Engine'], color='#69b3a2', width=0.5, linewidth=2, fliersize=5)
sns.stripplot(x=df['Engine'], color='#404040', alpha=0.3, size=4, jitter=True)

plt.title('Engine: BoxPlot', fontsize=15, pad=20)
plt.xlabel('Engine (CC)', fontsize=12)

sns.despine(left=True)

plt.tight_layout()
plt.show()


plt.figure(figsize=(10, 6))
sns.ecdfplot(data=df, x='Engine', color='#69b3a2', linewidth=2)
plt.title('Distribuzione Cumulativa (ECDF) di Engine', fontsize=14)
plt.xlabel('Engine')
plt.ylabel('Proporzione (0 a 1)')
plt.grid(True, alpha=0.3)
plt.show()














col = df['Power']

print(col.describe())

print("")

print("mode: ", col.mode())

print("")

print(f"Skewness: {col.skew():.2f}")
print(f"Kurtosis: {col.kurtosis():.2f}")





col = df['Power']

n = len(col)
rice_bins = int(2 * n**(1/3))

print(f"Num. of bins: {rice_bins}")

plt.figure(figsize=(17, 7))


ax = sns.histplot(
    x=col, 
    bins=rice_bins, 
    kde=True, 
    stat='probability',
    color="#108564",
    edgecolor='white'
)
plt.title(f'Distribuzione di Power')
plt.ylabel('Frequenza Relativa')
plt.xlabel('Power (bhp)')

plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 4))

sns.set_style("whitegrid")

ax = sns.boxplot(x=df['Power'], color='#69b3a2', width=0.5, linewidth=2, fliersize=5)
sns.stripplot(x=df['Power'], color='#404040', alpha=0.3, size=4, jitter=True)

plt.title('Power: BoxPlot', fontsize=15, pad=20)
plt.xlabel('Power (bhp)', fontsize=12)

sns.despine(left=True)

plt.tight_layout()
plt.show()


plt.figure(figsize=(10, 6))
sns.ecdfplot(data=df, x='Power', color='#69b3a2', linewidth=2)
plt.title('Distribuzione Cumulativa (ECDF) di Power', fontsize=14)
plt.xlabel('Power')
plt.ylabel('Proporzione (0 a 1)')
plt.grid(True, alpha=0.3)
plt.show()








tabella_brand = pd.crosstab(index=df['Cat'], columns='Frequenza Assoluta')
tabella_brand['Frequenza Relativa (%)'] = (tabella_brand['Frequenza Assoluta'] / tabella_brand['Frequenza Assoluta'].sum() * 100).round(2)
tabella_brand = tabella_brand.sort_values(by='Frequenza Assoluta', ascending=False)
tabella_brand['Frequenza Cumulata (%)'] = tabella_brand['Frequenza Relativa (%)'].cumsum().round(2)

display(tabella_brand)


count_assoluto = df['Cat'].value_counts()
count_relativo = (df['Cat'].value_counts(normalize=True) * 100).loc[count_assoluto.index] 

plt.figure(figsize=(17, 6))

ax = sns.barplot(x=count_assoluto.index, y=count_assoluto.values, hue=count_assoluto.index, palette='viridis', legend=False)

for i, container in enumerate(ax.containers):
    percentuale = count_relativo.iloc[i] 
    
    ax.bar_label(container, labels=[f'{percentuale:.2f}%'], padding=3, fontsize=10)

plt.title('Cat: Frequenze Assolute con Percentuale')
plt.xticks(rotation=45)
plt.xlabel('Cat')

plt.tight_layout()
plt.show()





df.head()














col = df['Price']

print(col.describe())

print("")

print("mode: ", col.mode())

print("")

print(f"Skewness: {col.skew():.2f}")
print(f"Kurtosis: {col.kurtosis():.2f}")





col = df['Price']

n = len(col)
rice_bins = int(2 * n**(1/3))

print(f"Num. of bins: {rice_bins}")

plt.figure(figsize=(17, 7))


ax = sns.histplot(
    x=col, 
    bins=rice_bins, 
    kde=True, 
    stat='probability',
    color="#108564",
    edgecolor='white'
)
plt.title(f'Distribuzione di Price')
plt.ylabel('Frequenza Relativa')
plt.xlabel('Price (Lahks)')

plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 4))

sns.set_style("whitegrid")

ax = sns.boxplot(x=df['Price'], color='#69b3a2', width=0.5, linewidth=2, fliersize=5)
sns.stripplot(x=df['Price'], color='#404040', alpha=0.3, size=4, jitter=True)

plt.title('Price: BoxPlot', fontsize=15, pad=20)
plt.xlabel('Price (Lahks)', fontsize=12)

sns.despine(left=True)

plt.tight_layout()
plt.show()


plt.figure(figsize=(10, 6))
sns.ecdfplot(data=df, x='Price', color='#69b3a2', linewidth=2)
plt.title('Distribuzione Cumulativa (ECDF) di Price', fontsize=14)
plt.xlabel('Price')
plt.ylabel('Proporzione (0 a 1)')
plt.grid(True, alpha=0.3)
plt.show()














df['Price'] = np.log1p(df['Price'])
df.head()











vars_continue = ['Age', 'Kilometers_Driven', 'Mileage', 'Engine', 'Power', 'Price']
df_corr = df[vars_continue].rename(columns={'Kilometers_Driven': 'K_Driven'}).corr()


plt.figure(figsize=(8, 6))

sns.heatmap(df_corr,
            annot=True,          
            fmt=".2f",           
            cmap='viridis',      
            vmin=-1, vmax=1,     
            center=0,            
            square=True,         
            linewidths=.5) 

plt.title('Matrice di Correlazione di Pearson', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()











vars_continue = ['Age', 'Kilometers_Driven', 'Mileage', 'Engine', 'Power', 'Price']

sns.set_theme(style="whitegrid")

g = sns.pairplot(df[vars_continue],
                 kind='kde', 
                 diag_kind='kde', 
                 plot_kws={'fill': True, 'color': '#21918c', 'thresh': 0.05, 'bw_adjust': 1.1}, 
                 diag_kws={'color': '#21918c', 'fill': True, 'bw_adjust': 1.1})

g.fig.suptitle('Matrice di Densità tra Variabili Continue', y=1.02, fontsize=14, fontweight='bold')

plt.show()














var = 'Price'
hue = 'Owner_Type'

plt.figure(figsize=(17, 7))

ax = sns.histplot(
    data=df,
    x=var,
    hue=hue,
    stat="probability",
    kde=True,
    fill=True,
    element="poly",
    common_norm=False,
    palette="viridis"
)

plt.title(f'Distribuzione di {var} per {hue}')
plt.ylabel('Frequenza Relativa')
plt.xlabel(var)

plt.tight_layout()
plt.show()

group_var = 'Transmission'
target = 'Price'

for group in df[group_var].unique():
    col_group = df[df[group_var] == group][target]
    
    print(f"--- Statistiche per {group_var}: {group} ---")
    
    print(f"Media:     {col_group.mean():.2f}")
    print(f"Mediana:   {col_group.median():.2f}")
    print(f"IQR:       {col_group.quantile(0.75) - col_group.quantile(0.25):.2f}")
    print(f"Std:       {col_group.std():.2f}")
    print("")
    
    print(f"Skewness: {col_group.skew():.2f}")
    print(f"Kurtosis: {col_group.kurtosis():.2f}")
    
    print("\n" + "="*45 + "\n")














from scipy import stats
from scipy.stats import normaltest
import statsmodels.api as sm


print(f"Gruppi di trasmissione: {df['Transmission'].unique()}\n")

group_per_transmission =  df.groupby('Transmission')['Price']
mean = group_per_transmission.mean()
std = group_per_transmission.std()

count = group_per_transmission.count()
std_err = std/np.sqrt(count)

confidence_level = 0.95

interval_manual = stats.norm.interval(confidence_level, loc=mean['Manual'], scale=std_err['Manual'])
interval_automatic = stats.norm.interval(confidence_level, loc=mean['Automatic'], scale=std_err['Automatic'])

print("---- Media per gruppo di tramissione ----")
print(f"Manual:\t\t {mean['Manual']:.2f}\nAutomatic:\t {mean['Automatic']:.2f}\n")

print("---- Std Dev per gruppo di tramissione ----")
print(f"Manual:\t\t {std['Manual']:.2f}\nAutomatic:\t {std['Automatic']:.2f}\n")

print("---- Std Err per gruppo di tramissione ----")
print(f"Manual:\t\t {std_err['Manual']:.2f}\nAutomatic:\t {std_err['Automatic']:.2f}\n")

print("---- Confidence Interval di price per gruppo di tramissione ----")
print(f"Manual:\t\t ({interval_manual[0]:.2f}, {interval_manual[1]:.2f})")
print(f"Automatic:\t ({interval_automatic[0]:.2f}, {interval_automatic[1]:.2f})\n")





manual = df[df['Transmission'] == 'Manual']['Price']
automatic = df[df['Transmission'] == 'Automatic']['Price']

t_stat, p_value = stats.ttest_ind(manual, automatic)

alpha = 0.05

print(f"Test statistic: {t_stat:0.2f}")
print(f"Significance level: {alpha:0.2f}")
print(f"P-value: {p_value:0.2f}")

if p_value < alpha:
    print("Conclusione: c'è una differenza significativa tra la media del prezzo delle auto Automatiche e Manuali.")
else:
    print("Conclusione: non c'è una differenza significativa tra la media del prezzo delle auto Automatiche e Manuali.")





statistic, p_value = normaltest(df['Price'])

alpha = 0.05

print(f"Test statistic: {statistic:.2f}")
print(f"P-value: {p_value:.10f}")

if p_value > alpha:
    print("E' Gaussiano. L'ipotesi H0 è vera.")
else:
    print("H0 rifiutata")





sm.qqplot(df['Price'], fit=True, line='45')
plt.grid()
plt.show()








group_per_transmission =  df.groupby('Transmission')['Price']
count = group_per_transmission.count()

total = df['Transmission'].count()

confidence_level = 0.95

proportion = count['Manual'] / total
conf_interval_man = sm.stats.proportion_confint(count['Manual'], total, alpha=1-confidence_level, method='normal')
conf_interval_aut = sm.stats.proportion_confint(count['Automatic'], total, alpha=1-confidence_level, method='normal')

print(f"---- Sample Proportion ----")
print(f"Manual:\t\t {proportion:.2f}")
print(f"Automatic:\t {1 - proportion:.2f}\n")

print("---- Confidence Interval of Proportion ----")
print(f"Manual:\t\t ({conf_interval_man[0]:.2f}, {conf_interval_man[1]:.2f})")
print(f"Automatic:\t ({conf_interval_aut[0]:.2f}, {conf_interval_aut[1]:.2f})\n")





group_per_transmission =  df.groupby('Transmission')['Price']
count = group_per_transmission.count()

total_count = count['Manual'] + count['Automatic']

null_hypothesis = 0.50
alpha = 0.05

z_stat, p_value = sm.stats.proportions_ztest(count=count['Manual'], nobs=total_count,  value=null_hypothesis)

prop_manual = count['Manual'] / total_count

print("---- Test Z per proporzione Trasmissione ----")
print(f"Proporzione Ipotizzata:\t {null_hypothesis}")
print(f"Proporzione Manuale: {prop_manual:.4f}\n")

print(f"Z-Test statistic: {z_stat:0.2f}")
print(f"Significance level: {alpha:0.2f}")
print(f"P-value: {p_value:0.2f}")

if p_value < alpha:
    print("Conclusione: c'è una differenza significativa tra le proporzioni di auto Manuali e Automatiche.")
else:
    print("Conclusione: non c'è una differenza significativa tra le proporzioni di auto Manuali e Automatiche.")





print(f"Gruppi di proprietario: {df['Owner_Type'].unique()}\n")

group_per_owner =  df.groupby('Owner_Type')['Price']
mean = group_per_owner.mean()
std = group_per_owner.std()

count = group_per_owner.count()
std_err = std/np.sqrt(count)

confidence_level = 0.95

interval_firt_ow = stats.norm.interval(confidence_level, loc=mean['First'], scale=std_err['First'])
interval_second_ow = stats.norm.interval(confidence_level, loc=mean['Second'], scale=std_err['Second'])
interval_third_ow = stats.norm.interval(confidence_level, loc=mean['Third'], scale=std_err['Third'])

print("---- Media per gruppo di Owner_Type ----")
print(f"Fisrt:\t\t {mean['First']:.2f}")
print(f"Second:\t\t {mean['Second']:.2f}")
print(f"Third:\t\t {mean['Third']:.2f}\n")

print("---- Std Dev per gruppo di Owner_Type ----")
print(f"First:\t\t {std['First']:.2f}")
print(f"Second:\t\t {std['Second']:.2f}")
print(f"Third:\t\t {std['Third']:.2f}\n")

print("---- Std Err per gruppo di tramissione ----")
print(f"First:\t\t {std_err['First']:.2f}")
print(f"Second:\t\t {std_err['Second']:.2f}")
print(f"Third:\t\t {std_err['Third']:.2f}\n")

print("---- Confidence Interval di price per gruppo di tramissione ----")
print(f"First:\t\t ({interval_firt_ow[0]:.2f}, {interval_firt_ow[1]:.2f})")
print(f"Second:\t\t ({interval_second_ow[0]:.2f}, {interval_second_ow[1]:.2f})")
print(f"Third:\t\t ({interval_third_ow[0]:.2f}, {interval_third_ow[1]:.2f})\n")





first = df[df['Owner_Type'] == 'First']['Price']
second = df[df['Owner_Type'] == 'Second']['Price']

t_stat, p_value = stats.ttest_ind(first, second)

alpha = 0.05

print(f"Test statistic: {t_stat:0.2f}")
print(f"Significance level: {alpha:0.2f}")
print(f"P-value: {p_value:0.2f}")

if p_value < alpha:
    print("Conclusione: c'è una differenza significativa tra la media del prezzo delle auto First owner rispetto alle Second owner.")
else:
    print("Conclusione: non c'è una differenza significativa tra la media del prezzo delle auto First owner rispetto alle Second owner.")





df.to_csv("dataset_for_modeling.csv", index=False)

df.head()


df['Owner_Type'].unique()
































y_modeling = df['Price'].copy()
y_modeling = y_modeling.rename('log_Price')

vars_da_escludere = ['Price', 'Model']
X_modeling = df.drop(columns=vars_da_escludere).copy()

owner_mapping = {'First': 1, 'Second': 2, 'Third': 3}
X_modeling['Owner_Type'] = X_modeling['Owner_Type'].map(owner_mapping)

vars_to_dummy = ['Brand', 'Location', 'Fuel_Type', 'Transmission', 'Cat']
X_modeling = pd.get_dummies(X_modeling, columns=vars_to_dummy, drop_first=True, dtype=int)








X_modeling['log_Power'] = np.log1p(X_modeling['Power'])
X_modeling['log_New_Price'] = np.log1p(X_modeling['New_Price_num'])
X_modeling['log_Kms'] = np.log1p(X_modeling['Kilometers_Driven'])

X_modeling = X_modeling.drop(['Engine', 'Power', 'New_Price_num', 'Kilometers_Driven'], axis=1)








import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm


y_inference = y_modeling.copy()
X_inference = X_modeling.copy()

X_inference = sm.add_constant(X_inference)
model_slr = sm.OLS(y_inference, X_inference).fit()

print(model_slr.summary())























p_values = model_slr.pvalues

significativi = p_values[p_values < 0.05].index.tolist()
non_significativi = p_values[p_values >= 0.05].index.tolist()

count_significativi = len(significativi)
count_non_significativi = len(non_significativi)

print(f"--- RISULTATI ANALISI P-VALUE (alpha: 0.05) ---\n")
print(f"Predittori STATISTICAMENTE SIGNIFICATIVI: {count_significativi}")
print(f"Nomi: {significativi}")
print(f"\nPredittori NON SIGNIFICATIVI: {count_non_significativi}")
print(f"Nomi: {non_significativi}")











params = model_slr.params
p_values = model_slr.pvalues
conf_int = model_slr.conf_int()

ci_low = conf_int[0]
ci_high = conf_int[1]

vars_elasticity = ['log_Kms', 'log_Power', 'log_New_Price']

df_elast = pd.DataFrame({
    'Coefficiente (beta)': params[vars_elasticity].round(4),
    'P-Value': p_values[vars_elasticity].round(4),
    'Impatto % su Prezzo': params[vars_elasticity].round(2),
    'CI Lower %': ci_low[vars_elasticity].round(2),
    'CI Upper %': ci_high[vars_elasticity].round(2)
})


vars_semi_elast = ['Age', 'Mileage', 'Owner_Type', 'Has_New_Price']

df_semi = pd.DataFrame({
    'Coefficiente (beta)': params[vars_semi_elast].round(4),
    'P-Value': p_values[vars_semi_elast].round(4),
    'Impatto % su Prezzo': ((np.exp(params[vars_semi_elast]) - 1) * 100).round(2),
    'CI Lower %': ((np.exp(ci_low[vars_semi_elast]) - 1) * 100).round(2),
    'CI Upper %': ((np.exp(ci_high[vars_semi_elast]) - 1) * 100).round(2)
})

# Unione e Ordinamento
df_continue_final = pd.concat([df_elast, df_semi])
df_continue_final['Abs_Impatto'] = df_continue_final['Impatto % su Prezzo'].abs()
df_continue_final = df_continue_final.sort_values(by='Abs_Impatto', ascending=False).drop(columns='Abs_Impatto')


df_categorie = {}
baseline_info = {} 

prefissi = ['Location', 'Brand', 'Fuel_Type', 'Transmission', 'Cat']

for pref in prefissi:
    
    group_vars = [v for v in params.index if v.startswith(pref + "_")]
    
    if not group_vars:
        continue
        
    data = []
    
    for var in group_vars:
        beta = params[var]
        low = ci_low[var]
        high = ci_high[var]
        p_val = p_values[var]
        
        data.append({
            'Categoria': var.replace(pref + "_", ""),
            'Coefficiente (beta)': round(beta, 4),
            'P-Value': round(p_val, 4),
            'Impatto %': round((np.exp(beta) - 1) * 100, 2),
            'CI Lower %': round((np.exp(low) - 1) * 100, 2),
            'CI Upper %': round((np.exp(high) - 1) * 100, 2),
            'Nota': 'Significativo' if p_val < 0.05 else 'Non Sig.'
        })
    
    
    temp_df = pd.DataFrame(data)
    
    row_baseline = {
        'Categoria': 'BASELINE (Riferimento)', 
        'Coefficiente (beta)': 0.0,
        'P-Value': 0.0, 
        'Impatto %': 0.0, 
        'CI Lower %': 0.0, 
        'CI Upper %': 0.0,
        'Nota': 'Riferimento'
    }
    
    temp_df = pd.concat([pd.DataFrame([row_baseline]), temp_df], ignore_index=True)
    
    temp_df['Abs_Impatto'] = temp_df['Impatto %'].abs()
    df_sorted = temp_df.iloc[1:].sort_values(by='Abs_Impatto', ascending=False)
    df_final_cat = pd.concat([temp_df.iloc[[0]], df_sorted]).drop(columns='Abs_Impatto')
    
    df_categorie[pref] = df_final_cat








print("Variabili Continue:")
display(df_continue_final)








print("\nImpatto Fuel_Type (Ordinato):")
display(df_categorie['Fuel_Type'])








print("\nImpatto Transmission:")
display(df_categorie['Transmission'])








print("\nImpatto Brand:")
display(df_categorie['Brand'])








print("\nImpatto Location (Ordinato):")
display(df_categorie['Location'])








print("\nImpatto Category:")
display(df_categorie['Cat'])











fitted_values = model_slr.fittedvalues
residuals = model_slr.resid

plt.figure(figsize=(12, 5))

# 1. Residuals vs. Fitted Plot
ax1 = plt.subplot(1, 2, 1)
sns.scatterplot(x=fitted_values, y=residuals, ax=ax1)
ax1.axhline(0, color='red', linestyle='--')
ax1.set_title('Residuals vs. Fitted')
ax1.set_xlabel('Fitted Values (Predicted Price)')
ax1.set_ylabel('Residuals')

# 2. Q-Q Plot
ax2 = plt.subplot(1, 2, 2)
sm.qqplot(residuals, line='s', ax=ax2)
ax2.set_title('Q-Q Plot of Residuals')

plt.tight_layout()
plt.show()









































X = X_modeling.copy()
y = y_modeling.copy()


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Sets Proportions:")
print(f"X:\t\t{X.shape}\t Prop: {X.shape[0]/X.shape[0]:.1f}")
print(f"X_train:\t{X_train.shape}\t Prop: {X_train.shape[0]/X.shape[0]:.1f}")
print(f"X_test:\t\t{X_test.shape}\t Prop: {X_test.shape[0]/X.shape[0]:.1f}")





from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)














from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import root_mean_squared_error





# Fissiamo il grado a 1 per la regressione lineare semplice
degree_lr = 1

metrics = {
    'MAE': 'neg_mean_absolute_error',
    'MSE': 'neg_mean_squared_error',
    'RMSE': 'neg_root_mean_squared_error',
    'R2': 'r2'
}

# REGRESSIONE LINEARE SEMPLICE
pipe_lr = Pipeline([
    ('polylr', PolynomialFeatures(degree=1)), # Grado 1 = Lineare Semplice
    ('modellr', LinearRegression())
])

param_grid = {}

gs_lr = GridSearchCV(pipe_lr, param_grid=param_grid, cv=5, scoring=metrics, refit='MSE')
gs_lr.fit(X_train, y_train)

best_idx_lr = gs_lr.best_index_
res_lr = gs_lr.cv_results_

auto_usate_result = pd.DataFrame({
    'Method': ['Linear Simple'],
    'Validation': ['K-Fold CV: 5'],
    'R2': [res_lr['mean_test_R2'][best_idx_lr]],
    'MAE': [res_lr['mean_test_MAE'][best_idx_lr] * -1],
    'MSE': [res_lr['mean_test_MSE'][best_idx_lr] * -1],
    'RMSE': [res_lr['mean_test_RMSE'][best_idx_lr] * -1]
})

# RIDGE LINEAR REGRESSION
ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}

ridge_gs = GridSearchCV(Ridge(), ridge_params, cv=5, scoring=metrics, refit='MSE')
ridge_gs.fit(X_train, y_train)

best_alpha_ridge = ridge_gs.best_params_['alpha'] 
best_idx_ridge = ridge_gs.best_index_
res_ridge = ridge_gs.cv_results_

ridge = pd.DataFrame({
    'Method': ['Linear Ridge'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Alpha: {}".format(best_alpha_ridge)],
    'R2': [res_ridge['mean_test_R2'][best_idx_ridge]],
    'MAE': [res_ridge['mean_test_MAE'][best_idx_ridge] * -1],
    'MSE': [res_ridge['mean_test_MSE'][best_idx_ridge] * -1],
    'RMSE': [res_ridge['mean_test_RMSE'][best_idx_ridge] * -1]
})
auto_usate_result = pd.concat([auto_usate_result, ridge], ignore_index=True)

# LASSO REGRESSION 
lasso_param_grid = {
    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10] 
}

gs_lasso = GridSearchCV(Lasso(max_iter=10000), param_grid=lasso_param_grid, cv=5, scoring=metrics, refit='MSE')
gs_lasso.fit(X_train, y_train)

best_alpha_lasso = gs_lasso.best_params_['alpha']
best_idx_lasso = gs_lasso.best_index_
res_lasso = gs_lasso.cv_results_

new_row_lasso = pd.DataFrame({
    'Method': ['Linear Lasso'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Alpha: {}".format(best_alpha_lasso)],
    'R2': [res_lasso['mean_test_R2'][best_idx_lasso]],
    'MAE': [res_lasso['mean_test_MAE'][best_idx_lasso] * -1],
    'MSE': [res_lasso['mean_test_MSE'][best_idx_lasso] * -1],
    'RMSE': [res_lasso['mean_test_RMSE'][best_idx_lasso] * -1]
})
auto_usate_result = pd.concat([auto_usate_result, new_row_lasso], ignore_index=True)

auto_usate_result














metrics = {
    'MAE': 'neg_mean_absolute_error',
    'MSE': 'neg_mean_squared_error',
    'RMSE': 'neg_root_mean_squared_error',
    'R2': 'r2'
}

# Ricerca del grado migliore
find_degree_pipe = Pipeline([
    ('poly', PolynomialFeatures()),
    ('model', Ridge())
])

degree_grid = {'poly__degree': [1, 2, 3]}

gs_degree = GridSearchCV(find_degree_pipe, param_grid=degree_grid, cv=5, scoring='neg_mean_squared_error')
gs_degree.fit(X_train, y_train)

best_degree = gs_degree.best_params_['poly__degree']
print("Grado ottimale individuato: {:>2}".format(best_degree))

# --- REGRESSIONE POLINOMIALE RIDGE ---
poly_ridge_pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=best_degree)),
    ('model', Ridge())
])

ridge_params = {'model__alpha': [0.1, 1, 10, 100, 200, 400, 600]}

gs_ridge = GridSearchCV(poly_ridge_pipe, param_grid=ridge_params, cv=5, scoring=metrics, refit='MSE')
gs_ridge.fit(X_train, y_train)

res_pr = gs_ridge.cv_results_
idx_pr = gs_ridge.best_index_
best_alpha_ridge = gs_ridge.best_params_['model__alpha']

new_row_ridge = pd.DataFrame({
    'Method': ['Polynomial Ridge'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}, Alpha: {}".format(best_degree, best_alpha_ridge)],
    'R2': [res_pr['mean_test_R2'][idx_pr]],
    'MAE': [res_pr['mean_test_MAE'][idx_pr] * -1],
    'MSE': [res_pr['mean_test_MSE'][idx_pr] * -1],
    'RMSE': [res_pr['mean_test_RMSE'][idx_pr] * -1]
})

auto_usate_result = pd.concat([auto_usate_result, new_row_ridge], ignore_index=True)

# --- REGRESSIONE POLINOMIALE LASSO ---
poly_lasso_pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=best_degree)),
    ('model', Lasso(max_iter=10000))
])

lasso_params = {'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]}

gs_lasso = GridSearchCV(poly_lasso_pipe, param_grid=lasso_params, cv=5, scoring=metrics, refit='MSE')
gs_lasso.fit(X_train, y_train)

res_pl = gs_lasso.cv_results_
idx_pl = gs_lasso.best_index_
best_alpha_lasso = gs_lasso.best_params_['model__alpha']

# Creazione riga per Polynomial Lasso
new_row_lasso = pd.DataFrame({
    'Method': ['Polynomial Lasso'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}, Alpha: {}".format(best_degree, best_alpha_lasso)],
    'R2': [res_pl['mean_test_R2'][idx_pl]],
    'MAE': [res_pl['mean_test_MAE'][idx_pl] * -1],
    'MSE': [res_pl['mean_test_MSE'][idx_pl] * -1],
    'RMSE': [res_pl['mean_test_RMSE'][idx_pl] * -1]
})

auto_usate_result = pd.concat([auto_usate_result, new_row_lasso], ignore_index=True)

# Cerchiamo il modello minore
best_model_idx = auto_usate_result['MSE'].idxmin()

best_model_name = auto_usate_result.loc[best_model_idx, 'Method']
best_model_mse = auto_usate_result.loc[best_model_idx, 'MSE']

print(f"Modello migliore: {best_model_name}")
print(f"MSE: {best_model_mse}")

auto_usate_result











best_model_test = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('model', Lasso(max_iter=10000, alpha=0.001))
])

best_model_test.fit(X_train, y_train)

y_test_pred = best_model_test.predict(X_test)

mae_test = mean_absolute_error(y_test, y_test_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = root_mean_squared_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

test_performance_result = pd.DataFrame({
    'Method': ['Best Model: Polynomial Lasso'],
    'Dataset': ['Test Set'],
    'Parameters': ['Degree: 2, Alpha: 0.001'],
    'R2': [r2_test],
    'MAE': [mae_test],
    'MSE': [mse_test],
    'RMSE': [rmse_test]
})

test_performance_result





























from sklearn.decomposition import PCA

pca_full = PCA()
pca_full.fit(X_train)

# Calcolo della varianza spiegata cumulativa
cum_variance = np.cumsum(pca_full.explained_variance_ratio_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cum_variance) + 1), cum_variance, marker='o', linestyle='--')
plt.axhline(y=0.90, color='r', linestyle='-', label='90% Varianza Spiegata')
plt.xlabel('Numero di Componenti')
plt.ylabel('Varianza Spiegata Cumulativa')
plt.title('Analisi della Varianza Spiegata (PCA)')
plt.grid(True)
plt.legend()
plt.show()

# Troviamo il numero esatto di componenti per il 90%
n_components_90 = np.argmax(cum_variance >= 0.90) + 1
print(f"Servono {n_components_90} componenti per spiegare il 90% della varianza originale.")











# Ricalcoliamo i due set
pca_opt = PCA(n_components=n_components_90)
X_train_pca = pca_opt.fit_transform(X_train)
X_test_pca = pca_opt.transform(X_test)


# Fissiamo il grado a 1 per la regressione lineare semplice
degree_lr = 1

# 1. Configurazione Metriche
metrics = {
    'MAE': 'neg_mean_absolute_error',
    'MSE': 'neg_mean_squared_error',
    'RMSE': 'neg_root_mean_squared_error',
    'R2': 'r2'
}

# --- 1. REGRESSIONE LINEARE SEMPLICE ---
pipeline_lr = Pipeline([
    ('poly', PolynomialFeatures(degree=degree_lr)),
    ('model', LinearRegression())
])

gs_lr = GridSearchCV(pipeline_lr, param_grid={}, cv=5, scoring=metrics, refit='MSE')
gs_lr.fit(X_train_pca, y_train)

best_idx_lr = gs_lr.best_index_
res_lr = gs_lr.cv_results_

new_slr_row = pd.DataFrame({
    'Method': ['Linear Simple PCA'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}".format(degree_lr)],
    'R2': [res_lr['mean_test_R2'][best_idx_lr]],
    'MAE': [res_lr['mean_test_MAE'][best_idx_lr] * -1],
    'MSE': [res_lr['mean_test_MSE'][best_idx_lr] * -1],
    'RMSE': [res_lr['mean_test_RMSE'][best_idx_lr] * -1]
})

auto_usate_result = pd.concat([auto_usate_result, new_slr_row], ignore_index=True)

# --- 2. RIDGE LINEAR REGRESSION ---
polynomial_regressor_ridge = Pipeline([
    ('polynomial_expansion', PolynomialFeatures(degree=degree_lr)),
    ('ridge_regression', Ridge())
])

ridge_param_grid = {
    'ridge_regression__alpha': [0.1, 1, 10, 50, 100, 200, 400, 600, 1000]
}

gs_ridge = GridSearchCV(polynomial_regressor_ridge, param_grid=ridge_param_grid, cv=5, scoring=metrics, refit='MSE')
gs_ridge.fit(X_train_pca, y_train)

best_alpha_ridge = gs_ridge.best_params_['ridge_regression__alpha']
best_idx_ridge = gs_ridge.best_index_
res_ridge = gs_ridge.cv_results_

new_row_ridge = pd.DataFrame({
    'Method': ['Linear Ridge PCA'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}, Alpha: {}".format(degree_lr, best_alpha_ridge)],
    'R2': [res_ridge['mean_test_R2'][best_idx_ridge]],
    'MAE': [res_ridge['mean_test_MAE'][best_idx_ridge] * -1],
    'MSE': [res_ridge['mean_test_MSE'][best_idx_ridge] * -1],
    'RMSE': [res_ridge['mean_test_RMSE'][best_idx_ridge] * -1]
})
auto_usate_result = pd.concat([auto_usate_result, new_row_ridge], ignore_index=True)

# --- 3. LASSO REGRESSION ---
polynomial_regressor_lasso = Pipeline([
    ('polynomial_expansion', PolynomialFeatures(degree=degree_lr)),
    ('lasso_regression', Lasso(max_iter=10000))
])

lasso_param_grid = {
    'lasso_regression__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]
}

gs_lasso = GridSearchCV(polynomial_regressor_lasso, param_grid=lasso_param_grid, cv=5, scoring=metrics, refit='MSE')
gs_lasso.fit(X_train_pca, y_train)

best_alpha_lasso = gs_lasso.best_params_['lasso_regression__alpha']
best_idx_lasso = gs_lasso.best_index_
res_lasso = gs_lasso.cv_results_

new_row_lasso = pd.DataFrame({
    'Method': ['Linear Lasso PCA'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}, Alpha: {}".format(degree_lr, best_alpha_lasso)],
    'R2': [res_lasso['mean_test_R2'][best_idx_lasso]],
    'MAE': [res_lasso['mean_test_MAE'][best_idx_lasso] * -1],
    'MSE': [res_lasso['mean_test_MSE'][best_idx_lasso] * -1],
    'RMSE': [res_lasso['mean_test_RMSE'][best_idx_lasso] * -1]
})
auto_usate_result = pd.concat([auto_usate_result, new_row_lasso], ignore_index=True)


# 1. Configurazione Metriche
metrics = {
    'MAE': 'neg_mean_absolute_error',
    'MSE': 'neg_mean_squared_error',
    'RMSE': 'neg_root_mean_squared_error',
    'R2': 'r2'
}

# Ricerca del grado migliore
find_degree_pipe = Pipeline([
    ('poly', PolynomialFeatures()),
    ('model', Ridge())
])

degree_grid = {'poly__degree': [1, 2, 3]}

gs_degree = GridSearchCV(find_degree_pipe, param_grid=degree_grid, cv=5, scoring='neg_mean_squared_error')
gs_degree.fit(X_train_pca, y_train)

best_degree = gs_degree.best_params_['poly__degree']
print("Grado ottimale individuato: {:>2}".format(best_degree))

# --- 4. REGRESSIONE POLINOMIALE SEMPLICE ---
poly_simple_pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=best_degree)),
    ('model', LinearRegression())
])

gs_poly_simple = GridSearchCV(poly_simple_pipe, param_grid={}, cv=5, scoring=metrics, refit='MSE')
gs_poly_simple.fit(X_train_pca, y_train)

res_ps = gs_poly_simple.cv_results_
idx_ps = gs_poly_simple.best_index_

# Creazione riga per Polynomial Simple
new_row_poly = pd.DataFrame({
    'Method': ['Polynomial Simple PCA'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}".format(best_degree)],
    'R2': [res_ps['mean_test_R2'][idx_ps]],
    'MAE': [res_ps['mean_test_MAE'][idx_ps] * -1],
    'MSE': [res_ps['mean_test_MSE'][idx_ps] * -1],
    'RMSE': [res_ps['mean_test_RMSE'][idx_ps] * -1]
})

auto_usate_result = pd.concat([auto_usate_result, new_row_poly], ignore_index=True)

# --- 5. REGRESSIONE POLINOMIALE RIDGE ---
poly_ridge_pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=best_degree)),
    ('model', Ridge())
])

ridge_params = {'model__alpha': [0.1, 1, 10, 50, 100, 200, 400, 600, 1000]}

gs_ridge = GridSearchCV(poly_ridge_pipe, param_grid=ridge_params, cv=5, scoring=metrics, refit='MSE')
gs_ridge.fit(X_train_pca, y_train)

res_pr = gs_ridge.cv_results_
idx_pr = gs_ridge.best_index_
best_alpha_ridge = gs_ridge.best_params_['model__alpha']

new_row_ridge = pd.DataFrame({
    'Method': ['Polynomial Ridge PCA'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}, Alpha: {}".format(best_degree, best_alpha_ridge)],
    'R2': [res_pr['mean_test_R2'][idx_pr]],
    'MAE': [res_pr['mean_test_MAE'][idx_pr] * -1],
    'MSE': [res_pr['mean_test_MSE'][idx_pr] * -1],
    'RMSE': [res_pr['mean_test_RMSE'][idx_pr] * -1]
})

auto_usate_result = pd.concat([auto_usate_result, new_row_ridge], ignore_index=True)

# --- 6. REGRESSIONE POLINOMIALE LASSO ---
poly_lasso_pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=best_degree)),
    ('model', Lasso(max_iter=10000))
])

lasso_params = {'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]}

gs_lasso = GridSearchCV(poly_lasso_pipe, param_grid=lasso_params, cv=5, scoring=metrics, refit='MSE')
gs_lasso.fit(X_train_pca, y_train)

res_pl = gs_lasso.cv_results_
idx_pl = gs_lasso.best_index_
best_alpha_lasso = gs_lasso.best_params_['model__alpha']

# Creazione riga per Polynomial Lasso
new_row_lasso = pd.DataFrame({
    'Method': ['Polynomial Lasso PCA'],
    'Validation': ['K-Fold CV: 5'],
    'Parameters': ["Degree: {}, Alpha: {}".format(best_degree, best_alpha_lasso)],
    'R2': [res_pl['mean_test_R2'][idx_pl]],
    'MAE': [res_pl['mean_test_MAE'][idx_pl] * -1],
    'MSE': [res_pl['mean_test_MSE'][idx_pl] * -1],
    'RMSE': [res_pl['mean_test_RMSE'][idx_pl] * -1]
})

auto_usate_result = pd.concat([auto_usate_result, new_row_lasso], ignore_index=True)






# Cerchiamo il modello minore
best_model_idx = auto_usate_result['MSE'].idxmin()

best_model_name = auto_usate_result.loc[best_model_idx, 'Method']
best_model_mse = auto_usate_result.loc[best_model_idx, 'MSE']

print(f"Modello migliore: {best_model_name}")
print(f"MSE: {best_model_mse}")

auto_usate_result












pca_2d = PCA(n_components=2)
X_pca_2d = pca_2d.fit_transform(X_train_pca)

plt.figure(figsize=(10, 8))
# Scatter plot: X=PC1, Y=PC2, Colore=Prezzo
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], 
                      c=y_train, cmap='viridis', alpha=0.5, s=15)

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA: Proiezione 2D del Dataset (Colorato per Prezzo)')
plt.colorbar(scatter, label='Price')
plt.grid(True)
plt.show()

















from sklearn.cluster import KMeans

X_for_clustering = X_train_pca 

inertia = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_for_clustering)
    inertia.append(kmeans.inertia_)

# Elbow plot
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o', linestyle='--')
plt.xlabel('Numero di Cluster (K)')
plt.title('Elbow Method per determinare K ottimale')
plt.grid(True)
plt.show()











k_scelto = 3
kmeans_final = KMeans(n_clusters=k_scelto, random_state=42, n_init=10)

cluster_labels = kmeans_final.fit_predict(X_train_pca)

# Plot
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], 
                      c=cluster_labels, cmap='viridis', alpha=0.6, s=15)

plt.xlabel('PC1 (Performance)')
plt.ylabel('PC2 (Usura)')
plt.title(f'Segmentazione Automatica: K-Means con {k_scelto} Cluster')
plt.colorbar(scatter, label='Cluster ID')
plt.grid(True)
plt.show()


df_train_analysis = X_train.copy() # Dati originali
df_train_analysis['Cluster'] = cluster_labels
df_train_analysis['Price_Real'] = np.exp(y_train) - 1 # Rimettiamo il prezzo in Lahk (da Log)

# Raggruppiamo per Cluster e vediamo le medie
profilo_cluster = df_train_analysis.groupby('Cluster')[['Price_Real']].mean()
profilo_cluster['Count'] = df_train_analysis['Cluster'].value_counts()

print("--- PROFILO DEI SEGMENTI DI MERCATO ---")
print(profilo_cluster.round(1))









